{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "\n",
    "import keras.metrics\n",
    "from keras import callbacks, models, metrics\n",
    "from datahandler.constants import all_features, data_version, acc_features, tensorboard_dir, location_labels, mag_features\n",
    "from datahandler.data_preprocessing import get_train_test_data, load_data_v3\n",
    "from models.log_writer import LogWriter\n",
    "from model.lstm import make_lstm_model_v1\n",
    "from model.transformer import make_transformer_model_v1, make_transformer_model_v2\n",
    "from model.cnn import make_cnn_model_v1, make_cnn_model_v2\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import print_line_divider\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datahandler.data_preprocessing_from_jupyter import load_train_test_data_raw_normalized,load_train_test_data_added_features_normalized, load_train_test_data_added_features_pca\n",
    "from audio.audio import play_training_is_complete\n",
    "from sklearn.metrics import classification_report\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "Preparing data...\n",
      "Loading from file: /Users/duc.letran/Desktop/FINAL PROJECT/context_transformer/data/v4/mix/mm5_datacollection.csv (1/25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DATA Loader\n",
    "print_line_divider()\n",
    "print(\"Preparing data...\")\n",
    "# x_train, y_train, x_test, y_test = get_train_test_data(supported_features, window_time_in_seconds, window_size)\n",
    "# x_train, y_train, x_test, y_test = load_data_v3(\n",
    "#     features = supported_features,\n",
    "#     window_time_in_seconds = window_time_in_seconds,\n",
    "#     window_size = window_size\n",
    "# )\n",
    "x_train, y_train, x_test, y_test = load_train_test_data_raw_normalized(selected_features=all_features)\n",
    "print(\"Train data shape: \" + str(x_train.shape) + \" | Train label shape: \" + str(y_train.shape))\n",
    "print(\"Test data shape: \" + str(x_test.shape) + \" | Test label shape: \" + str(y_test.shape))\n",
    "print_line_divider()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# LOGGING CONFIGURATION\n",
    "enabled_log = True\n",
    "enabled_tensor_board = True\n",
    "log_writer = LogWriter(enabled_log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING THE TRAINING PROCESS\n"
     ]
    }
   ],
   "source": [
    "# TRAINING CONFIGURATION\n",
    "print(\"STARTING THE TRAINING PROCESS\")\n",
    "SAVED_BEST_MODEL = \"model/best_model.h5\"\n",
    "window_time_in_seconds = 2\n",
    "window_size = 40\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "validation_split = 15 / 85\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.01,\n",
    "    name=\"Adam\",\n",
    ")\n",
    "loss_function = \"sparse_categorical_crossentropy\"\n",
    "supported_features = all_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "log_writer.write(\"Configuration\", line_divider=True)\n",
    "log_writer.write(\n",
    "    f\"\"\"Window size: {window_size}\n",
    "Window time in second: {window_time_in_seconds}\n",
    "Maximum number of epochs: {epochs}\n",
    "Batch size: {batch_size}\n",
    "Validation split: {validation_split}\n",
    "Optimizer: {optimizer}\n",
    "Loss function: {loss_function}\"\"\"\n",
    ")\n",
    "log_writer.write(\"Data\", line_divider=True)\n",
    "log_writer.write(\"Data loaded from version \" + data_version)\n",
    "log_writer.write(\n",
    "    f\"\"\"Data training shape: ${x_train.shape}\n",
    "Data testing shape: ${x_test.shape}\"\"\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "Distribution for set Training set\n",
      "\n",
      "holdinginhand count is 1285\n",
      "insidethepantpocket count is 1288\n",
      "calling count is 1303\n",
      "beingusedinhand count is 1302\n",
      "insidethebag count is 1279\n",
      "****************************************************\n",
      "Distribution for set Testing set\n",
      "\n",
      "holdinginhand count is 215\n",
      "insidethepantpocket count is 215\n",
      "calling count is 198\n",
      "beingusedinhand count is 199\n",
      "insidethebag count is 221\n"
     ]
    }
   ],
   "source": [
    "def check_distribution(labels_list, set_name):\n",
    "    label_count_dict = {}\n",
    "    for label_ind in labels_list:\n",
    "        if label_ind in label_count_dict.keys():\n",
    "            label_count_dict[label_ind] += 1\n",
    "        else:\n",
    "            label_count_dict[label_ind] = 1\n",
    "\n",
    "    print_line_divider()\n",
    "    print(\"Distribution for set \" + set_name + \"\\n\")\n",
    "    for key in label_count_dict:\n",
    "        key = int(key)\n",
    "        print(location_labels[key] + \" count is \" + str(label_count_dict[key]))\n",
    "\n",
    "\n",
    "check_distribution(y_train, \"Training set\")\n",
    "check_distribution(y_test, \"Testing set\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (40, 9)\n"
     ]
    }
   ],
   "source": [
    "if len(x_train.shape) == 2:\n",
    "    input_shape = (x_train.shape[1], 1)\n",
    "else:\n",
    "    input_shape = x_train.shape[1:]\n",
    "print(\"Input shape: \" + str(input_shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 40, 9)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 40, 9)       18          ['input_11[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 40, 9)       14985       ['layer_normalization_12[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 40, 9)        0           ['multi_head_attention_6[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  (None, 40, 9)       0           ['dropout_21[0][0]',             \n",
      " ambda)                                                           'input_11[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 40, 9)       18          ['tf.__operators__.add_12[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 40, 4)        40          ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 40, 4)        0           ['conv1d_27[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)             (None, 40, 9)        45          ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (TFOpL  (None, 40, 9)       0           ['conv1d_28[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 40, 9)       18          ['tf.__operators__.add_13[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 40, 9)       14985       ['layer_normalization_14[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 40, 9)        0           ['multi_head_attention_7[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (TFOpL  (None, 40, 9)       0           ['dropout_23[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 40, 9)       18          ['tf.__operators__.add_14[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)             (None, 40, 4)        40          ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 40, 4)        0           ['conv1d_29[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)             (None, 40, 9)        45          ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (TFOpL  (None, 40, 9)       0           ['conv1d_30[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_10 (G  (None, 40)          0           ['tf.__operators__.add_15[0][0]']\n",
      " lobalAveragePooling1D)                                                                           \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 128)          5248        ['global_average_pooling1d_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 128)          0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 5)            645         ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 36,105\n",
      "Trainable params: 36,105\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "****************************************************\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# SETTING UP THE MODEL\n",
    "# model_name, model = make_cnn_model_v1(input_shape=input_shape)\n",
    "# model_name, model = make_lstm_model_v1(input_shape=input_shape)\n",
    "model_name, model = make_transformer_model_v1(\n",
    "    input_shape=input_shape,\n",
    "    head_size=64,\n",
    "    num_heads=6,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=2,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "# model_name, model = make_transformer_model_v2(input_shape=input_shape)\n",
    "print(\"Model Summary:\")\n",
    "stringlist = []\n",
    "model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "short_model_summary = \"\\n\".join(stringlist)\n",
    "print(short_model_summary)\n",
    "print(print_line_divider())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# LOGGING THE MODEL\n",
    "log_writer.write(\"Model\", line_divider=True)\n",
    "log_writer.write(\"Model name: \" + model_name)\n",
    "log_writer.write(short_model_summary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "# MODELS CALLBACK AND SETUP\n",
    "callback_list = [\n",
    "    callbacks.ModelCheckpoint(SAVED_BEST_MODEL, save_best_only=True, monitor=\"val_loss\"),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=50, min_lr=0.0001),\n",
    "    callbacks.EarlyStopping(monitor=\"val_loss\", patience=200, verbose=1)\n",
    "]\n",
    "if enabled_tensor_board:\n",
    "    callback_list.append(callbacks.TensorBoard(log_dir=tensorboard_dir, histogram_freq=1))\n",
    "if log_writer.enabled:\n",
    "    callback_list.append(\n",
    "        callbacks.ModelCheckpoint(log_writer.base_folder + \"/model.h5\", save_best_only=True, monitor=\"val_loss\")\n",
    "    )\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_function,\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "Starting to train...\n",
      "Epoch 1/1000\n",
      " 24/167 [===>..........................] - ETA: 11s - loss: 1.5408 - sparse_categorical_accuracy: 0.3203"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "print_line_divider()\n",
    "print(\"Starting to train...\")\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callback_list,\n",
    "    validation_split=validation_split,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(\"Total training time in seconds: \" + str(training_time))\n",
    "print(\"Highest validation accuracy: \", max(history.history['val_sparse_categorical_accuracy']))\n",
    "log_writer.write(\"Result\", line_divider=True)\n",
    "log_writer.write(\"Training time: \" + str(end_time - start_time) + \" seconds.\")\n",
    "log_writer.write(\"Highest validation accuracy: \" + str(max(history.history['val_sparse_categorical_accuracy'])))\n",
    "play_training_is_complete()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if log_writer.enabled:\n",
    "    model.save(log_writer.base_folder + \"/model_last.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PLOTTING\n",
    "metric = \"sparse_categorical_accuracy\"\n",
    "plt.figure()\n",
    "plt.plot(history.history[metric])\n",
    "plt.plot(history.history[\"val_\" + metric])\n",
    "plt.title(\"model \" + model_name + \" :\" + metric)\n",
    "plt.ylabel(metric, fontsize=\"large\")\n",
    "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
    "if log_writer.enabled:\n",
    "    plt.savefig(os.path.join(log_writer.base_folder, \"Validation progress.png\"))\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "model = models.load_model(SAVED_BEST_MODEL)\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print_line_divider()\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)\n",
    "\n",
    "# Accuracy based on different labels\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "metrics_report = classification_report(y_test, y_pred)\n",
    "print(\"Metrics report: \")\n",
    "print(metrics_report)\n",
    "\n",
    "con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred).numpy()\n",
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=4)\n",
    "con_mat_df = pd.DataFrame(con_mat_norm, index=location_labels, columns=location_labels)\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(con_mat_df, annot=True, cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "if log_writer.enabled:\n",
    "    plt.savefig(os.path.join(log_writer.base_folder, \"Accuracy.png\"))\n",
    "plt.show()\n",
    "plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_writer.write(\"Test evaluation\", line_divider=True)\n",
    "log_writer.write(\"Test accuracy: \" + str(test_acc))\n",
    "log_writer.write(\"Test loss: \" + str(test_loss))\n",
    "log_writer.write(\"Metric report: \")\n",
    "log_writer.write(metrics_report)\n",
    "log_writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}