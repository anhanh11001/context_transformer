{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Step Detection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from datahandler.data_loader import load_data_from_file, load_date_from_steptracking_file\n",
    "from datahandler.constants import v4_walking, location_labels, v4_mix\n",
    "import os\n",
    "from utils import print_line_divider\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import random, seed\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load file names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ds5', 'mm3', 'pp1', 'tt3', 'ds4', 'pp5', 'ds3', 'mm5', 'mm4', 'tt1', 'pp3', 'tt5'])\n"
     ]
    }
   ],
   "source": [
    "# Loading file names\n",
    "data_folder = v4_mix\n",
    "\n",
    "\n",
    "def get_all_files_from_folder(folder):\n",
    "    file_names = []\n",
    "    for datafile in os.listdir(folder):\n",
    "        if datafile.startswith(\".\"):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(folder, datafile)\n",
    "        if os.path.isfile(path):\n",
    "            file_names.append(datafile)\n",
    "    return file_names\n",
    "\n",
    "\n",
    "data_dict = {}\n",
    "for filename in get_all_files_from_folder(data_folder):\n",
    "    code = filename[:3]\n",
    "    data_dict[code] = os.path.join(data_folder, filename)\n",
    "\n",
    "step_dict = {}\n",
    "for filename in get_all_files_from_folder(data_folder + \"/step_fixed\"):\n",
    "    code = filename[:3]\n",
    "    if code in data_dict.keys():\n",
    "        step_dict[code] = os.path.join(data_folder, \"step\", filename)\n",
    "\n",
    "print(step_dict.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Configuration\n",
    "WINDOW_SIZE = 40\n",
    "WINDOW_LENGTH_IN_SECONDS = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load data from one file pair"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# # Step 1 - Load data file\n",
    "# code = \"op1\"\n",
    "# df = load_data_from_file(data_dict[code])\n",
    "# df = df.drop(\"labelActivity\", axis=1)\n",
    "# df['labelPhone'] = df['labelPhone'].apply(lambda x: location_labels.index(x))\n",
    "# df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# # Step 2 - Load data file\n",
    "# step_dates = load_date_from_steptracking_file(step_dict[code])\n",
    "# print(\"There are \" + str(len(step_dates)) + \" steps found in this file\")\n",
    "# print(step_dates[int(len(step_dates) / 2)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# # Step 3 - Divide collected data into fixed-size chunk\n",
    "# fixed_size_data = []\n",
    "# fixed_size_indexes = []\n",
    "# current_timestamp = df.index[0].to_pydatetime()\n",
    "# last_timestamp_raw = df.index[df.shape[0] - 1].to_pydatetime()\n",
    "# current_timestamp_raw_index = 0\n",
    "# one_window_length_in_millis = WINDOW_LENGTH_IN_SECONDS * 1000 / WINDOW_SIZE\n",
    "# while True:\n",
    "#     current_timestamp = current_timestamp + timedelta(milliseconds=one_window_length_in_millis)\n",
    "#     if current_timestamp > last_timestamp_raw:\n",
    "#         break\n",
    "#\n",
    "#     while current_timestamp_raw_index < df.shape[0] - 1:\n",
    "#         next_timestamp_raw = df.index[current_timestamp_raw_index + 1].to_pydatetime()\n",
    "#         if next_timestamp_raw < current_timestamp:\n",
    "#             current_timestamp_raw_index += 1\n",
    "#         else:\n",
    "#             break\n",
    "#\n",
    "#     fixed_size_data.append(df.iloc[current_timestamp_raw_index])\n",
    "#     fixed_size_indexes.append(current_timestamp)\n",
    "# fixed_size_df = pd.DataFrame(\n",
    "#     data=fixed_size_data,\n",
    "#     index=fixed_size_indexes,\n",
    "#     columns=df.columns\n",
    "# )\n",
    "#\n",
    "# fixed_size_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# # Step 4 - Add step date to this fixed_size_df\n",
    "# def update_date_microsecond(date, first_rem, second_rem):\n",
    "#     current_microsecond = date.microsecond\n",
    "#     new_microsecond_one = (int(current_microsecond / 100000) * 100 + first_rem) * 1000\n",
    "#     new_microsecond_two = (int(current_microsecond / 100000) * 100 + second_rem) * 1000\n",
    "#     diff_one = abs(current_microsecond - new_microsecond_one)\n",
    "#     diff_two = abs(current_microsecond - new_microsecond_one)\n",
    "#     if diff_one > diff_two:\n",
    "#         new_microsecond = new_microsecond_two\n",
    "#     else:\n",
    "#         new_microsecond = new_microsecond_one\n",
    "#     return date.replace(microsecond=new_microsecond)\n",
    "#\n",
    "#\n",
    "# index_first_date = fixed_size_df.index[0]\n",
    "# print(type(index_first_date))\n",
    "# milli = index_first_date.microsecond / 1000\n",
    "# first_remainder = int(milli % 100)\n",
    "# second_remainder = int((milli + 50) % 100)\n",
    "#\n",
    "# updated_step_dates = set()\n",
    "# for date in step_dates:\n",
    "#     updated_step_dates.add(update_date_microsecond(date, first_remainder, second_remainder))\n",
    "#\n",
    "# is_step_series = []\n",
    "# count = 0\n",
    "# for m in fixed_size_df.index:\n",
    "#     is_step = m.to_pydatetime() in updated_step_dates\n",
    "#     if is_step:\n",
    "#         count += 1\n",
    "#     is_step_series.append(is_step)\n",
    "#\n",
    "# print(\"Found \" + str(count) + \" steps in this data frame.\")\n",
    "#\n",
    "# fixed_size_df['isStep'] = is_step_series\n",
    "# fixed_size_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# # Step 5 - Normalize the data\n",
    "#\n",
    "# def normalize(df):\n",
    "#     feature_count = df.shape[1] - 2\n",
    "#     values = df.iloc[:, 0:feature_count]\n",
    "#     phone_labels = df.loc[:, \"labelPhone\"].set_axis(range(df.shape[0]))\n",
    "#     is_step = df.loc[:, \"isStep\"].set_axis(range(df.shape[0]))\n",
    "#     normalizer = MinMaxScaler()\n",
    "#     normalized_values = normalizer.fit_transform(values)\n",
    "#     normalized_df = pd.DataFrame(\n",
    "#         data=normalized_values,\n",
    "#         columns=values.columns\n",
    "#     )\n",
    "#     normalized_df[\"labelPhone\"] = phone_labels\n",
    "#     normalized_df[\"isStep\"] = is_step\n",
    "#     return normalized_df\n",
    "#\n",
    "#\n",
    "# normalized_df = normalize(fixed_size_df)\n",
    "# normalized_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# # Step 6 - Slice the window\n",
    "#\n",
    "# def convert_df_to_final_train_data(df):\n",
    "#     train_x, test_x = [], []\n",
    "#     train_x_lite, test_x_lite = [], []\n",
    "#     train_y_context, test_y_context = [], []\n",
    "#     train_y_step, test_y_step = [], []\n",
    "#     window_index_start = 0\n",
    "#     window_index_increasing_size = 2\n",
    "#     no_features = df.shape[1] - 2\n",
    "#     values = df.iloc[:, 0:no_features]\n",
    "#     context_labels = df.loc[:, \"labelPhone\"]\n",
    "#     step_found = df.loc[:, \"isStep\"]\n",
    "#\n",
    "#     while window_index_start + WINDOW_SIZE < df.shape[0]:\n",
    "#         train_data = values[window_index_start:(window_index_start + WINDOW_SIZE)]\n",
    "#         train_data_lite = train_data[-2:]\n",
    "#         context_label = context_labels[window_index_start]\n",
    "#         is_step_list = step_found[window_index_start:(window_index_start + WINDOW_SIZE)].to_list()\n",
    "#         is_step = is_step_list[-1] or is_step_list[-2]\n",
    "#\n",
    "#         if random() < 0.15:\n",
    "#             test_x_lite.append(train_data_lite)\n",
    "#             test_x.append(train_data)\n",
    "#             test_y_context.append(context_label)\n",
    "#             test_y_step.append(is_step)\n",
    "#         else:\n",
    "#             train_x_lite.append(train_data_lite)\n",
    "#             train_x.append(train_data)\n",
    "#             train_y_context.append(context_label)\n",
    "#             train_y_step.append(is_step)\n",
    "#\n",
    "#         window_index_start += window_index_increasing_size\n",
    "#\n",
    "#     return np.array(train_x), np.array(train_x_lite), np.array(train_y_step), np.array(train_y_context), np.array(\n",
    "#         test_x), np.array(test_x_lite), np.array(test_y_step), np.array(test_y_context)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building model & configuration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def load_data_from_code(code):\n",
    "    # Step 1\n",
    "    df = load_data_from_file(data_dict[code])\n",
    "    df = df.drop(\"labelActivity\", axis=1)\n",
    "    df['labelPhone'] = df['labelPhone'].apply(lambda x: location_labels.index(x))\n",
    "    # Step 2\n",
    "    step_dates = load_date_from_steptracking_file(step_dict[code])\n",
    "    # Step 3\n",
    "    fixed_size_data = []\n",
    "    fixed_size_indexes = []\n",
    "    current_timestamp = df.index[0].to_pydatetime()\n",
    "    last_timestamp_raw = df.index[df.shape[0] - 1].to_pydatetime()\n",
    "    current_timestamp_raw_index = 0\n",
    "    one_window_length_in_millis = WINDOW_LENGTH_IN_SECONDS * 1000 / WINDOW_SIZE\n",
    "    while True:\n",
    "        current_timestamp = current_timestamp + timedelta(milliseconds=one_window_length_in_millis)\n",
    "        if current_timestamp > last_timestamp_raw:\n",
    "            break\n",
    "\n",
    "        while current_timestamp_raw_index < df.shape[0] - 1:\n",
    "            next_timestamp_raw = df.index[current_timestamp_raw_index + 1].to_pydatetime()\n",
    "            if next_timestamp_raw < current_timestamp:\n",
    "                current_timestamp_raw_index += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        fixed_size_data.append(df.iloc[current_timestamp_raw_index])\n",
    "        fixed_size_indexes.append(current_timestamp)\n",
    "    fixed_size_df = pd.DataFrame(\n",
    "        data=fixed_size_data,\n",
    "        index=fixed_size_indexes,\n",
    "        columns=df.columns\n",
    "    )\n",
    "\n",
    "    # Step 4\n",
    "    def update_date_microsecond(date, first_rem, second_rem):\n",
    "        current_microsecond = date.microsecond\n",
    "        new_microsecond_one = (int(current_microsecond / 100000) * 100 + first_rem) * 1000\n",
    "        new_microsecond_two = (int(current_microsecond / 100000) * 100 + second_rem) * 1000\n",
    "        diff_one = abs(current_microsecond - new_microsecond_one)\n",
    "        diff_two = abs(current_microsecond - new_microsecond_one)\n",
    "        if diff_one > diff_two:\n",
    "            new_microsecond = new_microsecond_two\n",
    "        else:\n",
    "            new_microsecond = new_microsecond_one\n",
    "        return date.replace(microsecond=new_microsecond)\n",
    "\n",
    "    index_first_date = fixed_size_df.index[0]\n",
    "    milli = index_first_date.microsecond / 1000\n",
    "    first_remainder = int(milli % 100)\n",
    "    second_remainder = int((milli + 50) % 100)\n",
    "\n",
    "    updated_step_dates = set()\n",
    "    for date in step_dates:\n",
    "        updated_step_dates.add(update_date_microsecond(date, first_remainder, second_remainder))\n",
    "\n",
    "    is_step_series = []\n",
    "    count = 0\n",
    "    for m in fixed_size_df.index:\n",
    "        is_step = m.to_pydatetime() in updated_step_dates\n",
    "        if is_step:\n",
    "            count += 1\n",
    "        is_step_series.append(is_step)\n",
    "\n",
    "    fixed_size_df['isStep'] = is_step_series\n",
    "\n",
    "    # Step 5\n",
    "    def normalize(df):\n",
    "        feature_count = df.shape[1] - 2\n",
    "        values = df.iloc[:, 0:feature_count]\n",
    "        phone_labels = df.loc[:, \"labelPhone\"].set_axis(range(df.shape[0]))\n",
    "        is_step = df.loc[:, \"isStep\"].set_axis(range(df.shape[0]))\n",
    "        normalizer = MinMaxScaler()\n",
    "        normalized_values = normalizer.fit_transform(values)\n",
    "        normalized_df = pd.DataFrame(\n",
    "            data=normalized_values,\n",
    "            columns=values.columns\n",
    "        )\n",
    "        normalized_df[\"labelPhone\"] = phone_labels\n",
    "        normalized_df[\"isStep\"] = is_step\n",
    "        return normalized_df\n",
    "\n",
    "    normalized_df = normalize(fixed_size_df)\n",
    "\n",
    "    # Step 6\n",
    "    def count_number_of_steps(list):\n",
    "        step_count = 0\n",
    "        for item in list:\n",
    "            if item:\n",
    "                step_count += 1\n",
    "        return step_count\n",
    "\n",
    "    def convert_df_to_final_train_data(df):\n",
    "\n",
    "        train_x, test_x = [], []\n",
    "        train_x_lite, test_x_lite = [], []\n",
    "        train_y_context, test_y_context = [], []\n",
    "        train_y_step, test_y_step = [], []\n",
    "\n",
    "        window_index_start = 0\n",
    "        window_index_increasing_size = int(WINDOW_SIZE / 8)  # 2\n",
    "        no_features = df.shape[1] - 2\n",
    "        values = df.iloc[:, 0:no_features]\n",
    "        context_labels = df.loc[:, \"labelPhone\"]\n",
    "        step_found = df.loc[:, \"isStep\"]\n",
    "\n",
    "        while window_index_start + WINDOW_SIZE < df.shape[0]:\n",
    "            train_data = values[window_index_start:(window_index_start + WINDOW_SIZE)]\n",
    "            train_data_lite = train_data[-2:]\n",
    "            context_label = context_labels[window_index_start]\n",
    "            step_count = count_number_of_steps(step_found[window_index_start:(window_index_start + WINDOW_SIZE)])\n",
    "            # is_step_list = step_found[window_index_start:(window_index_start + WINDOW_SIZE)]\n",
    "\n",
    "            if random() < 0.15:\n",
    "                test_x_lite.append(train_data_lite)\n",
    "                test_x.append(train_data)\n",
    "                test_y_context.append(context_label)\n",
    "                # test_y_step.append(is_step_list)\n",
    "                test_y_step.append(step_count)\n",
    "            else:\n",
    "                train_x_lite.append(train_data_lite)\n",
    "                train_x.append(train_data)\n",
    "                train_y_context.append(context_label)\n",
    "                # train_y_step.append(is_step_list)\n",
    "                train_y_step.append(step_count)\n",
    "\n",
    "            window_index_start += window_index_increasing_size\n",
    "\n",
    "        return train_x, train_x_lite, train_y_step, train_y_context, test_x, test_x_lite, test_y_step, test_y_context\n",
    "\n",
    "    return convert_df_to_final_train_data(normalized_df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling data from code tt3\n",
      "Handling data from code pp3\n",
      "Handling data from code ds5\n",
      "****************************************************\n",
      "(3082, 40, 9) (3082, 2, 9) (3082,) (3082,)\n",
      "(517, 40, 9) (517, 2, 9) (517,) (517,)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_x_lite, train_y_step, train_y_context = [], [], [], []\n",
    "test_x, test_x_lite, test_y_step, test_y_context = [], [], [], []\n",
    "\n",
    "# mm, tt, pp, ds, os\n",
    "inside_pant_pocket = [\"tt1\", \"pp1\", \"ds4\"] # 75%\n",
    "inside_the_bag = [\"tt5\", \"pp5\", \"ds3\"]\n",
    "swinging_in_hand = [\"tt3\", \"pp3\", \"ds5\"]\n",
    "for code in swinging_in_hand:\n",
    "    print(\"Handling data from code \" + code)\n",
    "    a, b, c, d, e, f, g, h = load_data_from_code(code)\n",
    "\n",
    "    train_x = train_x + a\n",
    "    train_x_lite = train_x_lite + b\n",
    "    train_y_step = train_y_step + c\n",
    "    train_y_context = train_y_context + d\n",
    "    test_x = test_x + e\n",
    "    test_x_lite = test_x_lite + f\n",
    "    test_y_step = test_y_step + g\n",
    "    test_y_context = test_y_context + h\n",
    "\n",
    "train_x = np.array(train_x)\n",
    "train_x_lite = np.array(train_x_lite)\n",
    "train_y_step = np.array(train_y_step)\n",
    "train_y_context = np.array(train_y_context)\n",
    "test_x = np.array(test_x)\n",
    "test_x_lite = np.array(test_x_lite)\n",
    "test_y_step = np.array(test_y_step)\n",
    "test_y_context = np.array(test_y_context)\n",
    "\n",
    "print_line_divider()\n",
    "print(train_x.shape, train_x_lite.shape, train_y_step.shape, train_y_context.shape)\n",
    "print(test_x.shape, test_x_lite.shape, test_y_step.shape, test_y_context.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Simple binary classifier using CNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 40, 9)]           0         \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 40, 64)            1792      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 40, 64)           256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_6 (ReLU)              (None, 40, 64)            0         \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 40, 64)            12352     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 40, 64)           256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_7 (ReLU)              (None, 40, 64)            0         \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,105\n",
      "Trainable params: 22,849\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models, Sequential\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def build_shallow_cnn(input_shape):\n",
    "    model_2d = Sequential()\n",
    "    model_2d.add(\n",
    "        layers.Conv1D(\n",
    "            filters=12,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation='relu',\n",
    "            input_shape=input_shape)\n",
    "    )\n",
    "    model_2d.add(layers.Dropout(0.1))\n",
    "    model_2d.add(layers.MaxPooling1D(pool_size=2, strides=2, padding=\"valid\"))\n",
    "    model_2d.add(layers.Flatten())\n",
    "    model_2d.add(layers.Dense(50, activation='relu'))\n",
    "    model_2d.add(layers.Dropout(0.4))\n",
    "    model_2d.add(layers.Dense(1, activation='relu'))\n",
    "    return model_2d\n",
    "\n",
    "\n",
    "def build_cnn_step_model(input_shape):\n",
    "    input_layer = layers.Input(input_shape)\n",
    "    conv1 = layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = layers.BatchNormalization()(conv1)\n",
    "    conv1 = layers.ReLU()(conv1)\n",
    "    conv2 = layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = layers.BatchNormalization()(conv2)\n",
    "    conv2 = layers.ReLU()(conv2)\n",
    "    conv3 = layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "    conv3 = layers.BatchNormalization()(conv3)\n",
    "    conv3 = layers.ReLU()(conv3)\n",
    "    gap = layers.GlobalAveragePooling1D()(conv2)\n",
    "    dense = layers.Dense(128, activation='relu')(gap)\n",
    "    output_layer = layers.Dense(1, activation=\"relu\")(dense)\n",
    "\n",
    "    return models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\n",
    "model = build_cnn_step_model((40, 9))\n",
    "# model = build_shallow_cnn((40,9))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 1.0760 - val_loss: 2.2541 - lr: 0.0050\n",
      "Epoch 2/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.6063 - val_loss: 2.3207 - lr: 0.0050\n",
      "Epoch 3/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5737 - val_loss: 1.5519 - lr: 0.0050\n",
      "Epoch 4/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5393 - val_loss: 1.8999 - lr: 0.0050\n",
      "Epoch 5/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5478 - val_loss: 1.0404 - lr: 0.0050\n",
      "Epoch 6/300\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.4861 - val_loss: 1.1340 - lr: 0.0050\n",
      "Epoch 7/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4845 - val_loss: 5.7941 - lr: 0.0050\n",
      "Epoch 8/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4318 - val_loss: 1.9868 - lr: 0.0050\n",
      "Epoch 9/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4468 - val_loss: 2.2246 - lr: 0.0050\n",
      "Epoch 10/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3807 - val_loss: 2.3859 - lr: 0.0050\n",
      "Epoch 11/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3714 - val_loss: 2.1297 - lr: 0.0050\n",
      "Epoch 12/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3891 - val_loss: 1.1895 - lr: 0.0050\n",
      "Epoch 13/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3547 - val_loss: 2.0351 - lr: 0.0050\n",
      "Epoch 14/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3836 - val_loss: 1.7947 - lr: 0.0050\n",
      "Epoch 15/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3462 - val_loss: 1.7897 - lr: 0.0050\n",
      "Epoch 16/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3307 - val_loss: 2.6763 - lr: 0.0050\n",
      "Epoch 17/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3132 - val_loss: 2.6336 - lr: 0.0050\n",
      "Epoch 18/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3116 - val_loss: 2.3814 - lr: 0.0050\n",
      "Epoch 19/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.3185 - val_loss: 3.2643 - lr: 0.0050\n",
      "Epoch 20/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3091 - val_loss: 3.0123 - lr: 0.0050\n",
      "Epoch 21/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2687 - val_loss: 1.3608 - lr: 0.0050\n",
      "Epoch 22/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2607 - val_loss: 3.1314 - lr: 0.0050\n",
      "Epoch 23/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2641 - val_loss: 4.0923 - lr: 0.0050\n",
      "Epoch 24/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2585 - val_loss: 3.0839 - lr: 0.0050\n",
      "Epoch 25/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2505 - val_loss: 3.9105 - lr: 0.0050\n",
      "Epoch 26/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2479 - val_loss: 3.2633 - lr: 0.0050\n",
      "Epoch 27/300\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.2503 - val_loss: 1.1979 - lr: 0.0050\n",
      "Epoch 28/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2259 - val_loss: 2.2371 - lr: 0.0050\n",
      "Epoch 29/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2366 - val_loss: 2.4499 - lr: 0.0050\n",
      "Epoch 30/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2280 - val_loss: 3.7941 - lr: 0.0050\n",
      "Epoch 31/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1883 - val_loss: 3.8145 - lr: 0.0050\n",
      "Epoch 32/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2013 - val_loss: 3.5031 - lr: 0.0050\n",
      "Epoch 33/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2165 - val_loss: 3.5413 - lr: 0.0050\n",
      "Epoch 34/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1884 - val_loss: 3.1728 - lr: 0.0050\n",
      "Epoch 35/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2063 - val_loss: 2.9503 - lr: 0.0050\n",
      "Epoch 36/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1888 - val_loss: 3.6679 - lr: 0.0050\n",
      "Epoch 37/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2059 - val_loss: 1.8479 - lr: 0.0050\n",
      "Epoch 38/300\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1794 - val_loss: 1.1071 - lr: 0.0050\n",
      "Epoch 39/300\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1699 - val_loss: 3.5598 - lr: 0.0050\n",
      "Epoch 40/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1777 - val_loss: 2.7424 - lr: 0.0050\n",
      "Epoch 41/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1798 - val_loss: 3.2600 - lr: 0.0050\n",
      "Epoch 42/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2186 - val_loss: 3.7916 - lr: 0.0050\n",
      "Epoch 43/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1941 - val_loss: 1.3024 - lr: 0.0050\n",
      "Epoch 44/300\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1602 - val_loss: 4.2666 - lr: 0.0050\n",
      "Epoch 45/300\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1565 - val_loss: 3.5129 - lr: 0.0050\n",
      "Epoch 46/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1798 - val_loss: 3.0955 - lr: 0.0050\n",
      "Epoch 47/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1574 - val_loss: 1.7859 - lr: 0.0050\n",
      "Epoch 48/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1452 - val_loss: 3.7063 - lr: 0.0050\n",
      "Epoch 49/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1364 - val_loss: 2.6541 - lr: 0.0050\n",
      "Epoch 50/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1596 - val_loss: 3.7763 - lr: 0.0050\n",
      "Epoch 51/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1613 - val_loss: 1.5938 - lr: 0.0050\n",
      "Epoch 52/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1526 - val_loss: 2.3417 - lr: 0.0050\n",
      "Epoch 53/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1421 - val_loss: 3.2362 - lr: 0.0050\n",
      "Epoch 54/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1882 - val_loss: 1.7348 - lr: 0.0050\n",
      "Epoch 55/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1486 - val_loss: 3.1490 - lr: 0.0050\n",
      "Epoch 56/300\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1178 - val_loss: 2.8981 - lr: 0.0025\n",
      "Epoch 57/300\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1219 - val_loss: 2.8729 - lr: 0.0025\n",
      "Epoch 58/300\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1194 - val_loss: 2.7420 - lr: 0.0025\n",
      "Epoch 59/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1203 - val_loss: 3.2819 - lr: 0.0025\n",
      "Epoch 60/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1170 - val_loss: 2.9341 - lr: 0.0025\n",
      "Epoch 61/300\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1196 - val_loss: 3.1890 - lr: 0.0025\n",
      "Epoch 62/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1144 - val_loss: 3.1873 - lr: 0.0025\n",
      "Epoch 63/300\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1069 - val_loss: 3.2188 - lr: 0.0025\n",
      "Epoch 64/300\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.1086 - val_loss: 3.7232 - lr: 0.0025\n",
      "Epoch 65/300\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1086 - val_loss: 3.3747 - lr: 0.0025\n",
      "Epoch 66/300\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.1204 - val_loss: 3.3112 - lr: 0.0025\n",
      "Epoch 67/300\n",
      "68/80 [========================>.....] - ETA: 0s - loss: 0.1104"
     ]
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "import tensorflow as tf\n",
    "\n",
    "SAVED_BEST_MODEL = \"model/best_step_model.h5\"\n",
    "callback_list = [\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=50, min_lr=0.0001),\n",
    "    # callbacks.EarlyStopping(monitor=\"val_loss\", patience=500, verbose=1),\n",
    "    callbacks.ModelCheckpoint(SAVED_BEST_MODEL, save_best_only=True, monitor=\"val_loss\")\n",
    "]\n",
    "epochs = 300\n",
    "batch_size = 32\n",
    "validation_split = 15 / 85\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.005,\n",
    "    name=\"Adam\",\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='mse',\n",
    "    # metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(\n",
    "    train_x,\n",
    "    train_y_step,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callback_list,\n",
    "    validation_split=validation_split,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "model = models.load_model(SAVED_BEST_MODEL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the result\n",
    "metric = \"loss\"\n",
    "plt.figure()\n",
    "plt.plot(history.history[metric])\n",
    "plt.plot(history.history[\"val_\" + metric])\n",
    "plt.title(\"CNN:\" + metric)\n",
    "plt.ylabel(metric, fontsize=\"large\")\n",
    "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def round_pred(pred):\n",
    "    round_list = []\n",
    "    for i in pred:\n",
    "        round_list.append(round(i[0]))\n",
    "    return round_list\n",
    "\n",
    "def print_correct_count(pred, actual):\n",
    "    old_pred = pred\n",
    "    pred = round_pred(pred)\n",
    "    correct_count = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == pred[i]:\n",
    "            print(\"Correct: \", pred[i])\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            print(\"Incorrect: index - \", i, \" | base - \", old_pred[i][0] ,\" | predict - \", pred[i], \" | actual - \", actual[i])\n",
    "    print(\"Correct: \", correct_count, \"/\", len(actual))\n",
    "\n",
    "\n",
    "y_pred = model.predict(test_x)\n",
    "print_correct_count(y_pred, test_y_step)\n",
    "\n",
    "print_line_divider()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}